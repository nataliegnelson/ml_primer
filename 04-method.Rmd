# Machine Learning Methods  

Here we provide a list of commonly used machine learning methods and some brief discussion.  

## Tree-based Methods  

Tree-based methods are popular for their ease for interpretation. Essentially, the algorithm tries to split the data into subsets by finding the most significant (or consequential) variables in a way that reduces the prediction error the most. For example, in the graph below, the variable that helps reduce prediction error at the first level is gender. In other words, the object being male or female can greatly help the algorithm predict the survival of passengers on Titanic. 

![Image Source: https://en.wikipedia.org/wiki/Decision_tree_learning](images/Decision_Tree.jpg)

Decision trees are easy to use but not very accurate for complex data; however, we can go beyond one tree. If we use multiple trees and, for instance, take the average of the predictions of all the trees, we can improve the performance. This is the idea of ensemble methods. One of the ensemble methods, random forests, is commonly used, including in natural resources management, and quite accurate for many kinds of data. So next we'll dive into this one method.  


### Random Forests  

For random forests, the algorithm randomly selects a smaller number, say $m$, of predictors from the total number, say $p$, each time a tree is being fit. This process is repeated many times, for example, $500$ times. The average (for continuous responses) or the majority votes (for discrete responses) of the predictions from the individual trees are used as final predictions. By randomly choosing a subset of predictors each time, random forests can avoid relying on the same variables every time and prevent overfitting.  

Even though coefficients of each predictors are not available for random forests, you can easily get the important variables and at least know which variables are more important for predicting the response. You can also use random forests as a tool for variable selection and then use a different method to fit the model on a subset of predictors chosen by random forests.   

[Need examples]  

### Other Ensemble Tree Methods  

Other ensemble tree methods include boosting and bagging. If you are interested, take a look at [a source].  

