[{"path":"index.html","id":"motivation","chapter":"1 Motivation","heading":"1 Motivation","text":"machine learning (ML) become powerful tool, noted ML widely used environmental studies. booklet meant provide concise guide natural resource management practitioners. book serves staring point rather comprehensive resource, practitioners can basic understanding ML works utilize analyze data answer research questions. appropriate, provide case studies R code well online resources help readers journey gaining one powerful tool seems omnipresent research world.","code":""},{"path":"intro.html","id":"intro","chapter":"2 Introduction","heading":"2 Introduction","text":"machine learning? Essentially, machine learning teaches computer models look patterns make predictions. might sound like magic might seem complicated, can think machine learning models finding underlying formulas data come . solve formula, many, many mathematical calculations involved. human beings prone mistakes, long can identify framework, can give framework data computer model. best repeating meticulous calculations find best guess based believes system data observed.Even though ML models commonly considered black boxes, necessarily . Many ML methods lend easily interpretation. Random forests, example, powerful method can offer glimpse important variables , even straightforward explanation can come linear model available. introduce two large branch machine learning. Even though environmental studies probably fall supervised learning, worth knowing unsupervised learning, might help intermediate steps studies.","code":""},{"path":"intro.html","id":"supervised-learning","chapter":"2 Introduction","heading":"2.1 Supervised Learning","text":"Simply put, supervised learning true answer model. example, want use environmental traits (temperature, precipitation, chemical composition) predict algal growth. dataset record activities, growths right answers. model can use predictors make predictions, comparison predictions truth can show well model performs.","code":""},{"path":"intro.html","id":"unsupervised-learning","chapter":"2 Introduction","heading":"2.2 Unsupervised Learning","text":"Unsupervised learning, hand, performs tasks correct answer. example, group chemicals, might similar traits others. Unsupervised learning can perform clustering find groupings, still unsupervised grouping depends context. One imagine clustering can performed first, groupings used predictors instead chemicals . way dimension can reduced, assuming number groups smaller number chemicals.","code":""},{"path":"data.html","id":"data","chapter":"3 Data","heading":"3 Data","text":"data, don’t feel rushed jump data analysis yet. steps can help know data better , often, avoid problems road.","code":""},{"path":"data.html","id":"data-exploratoary-analysis","chapter":"3 Data","heading":"3.1 Data Exploratoary Analysis","text":"read data, checks. use embedded mtcars example illustration.seen , command str allows see variables types. dataset, variables numerical. categorical numerical, make sure transform right type data. (Sometimes numbers can saved characters, analysis correct datatype remains character.)Next, try make plots–typically histograms numerical variables barplots categorical variables.can also look paired plots see two variables perfectly correlated, cause problems regression models.Next, take look summary statistics.summaries can show, example, ranges means variables. can give better understanding values whether might data entry errors.","code":"\nlibrary(tidyverse)## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.4\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\ndata(\"mtcars\")\nstr(mtcars)## 'data.frame':    32 obs. of  11 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\n##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n##  $ qsec: num  16.5 17 18.6 19.4 17 ...\n##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\npar(mfrow = c(3,3), mar = c(2,2,2,2))\nfor (i in 1:ncol(mtcars)){\n  hist(mtcars[,i], main = paste0(\"Histogram of \", colnames(mtcars)[i]))\n}\npairs(mtcars)\nfor (i in 1:ncol(mtcars)){\n  print(paste0(\"***** Summaries of \", colnames(mtcars)[i],\" *****\"))\n  print(summary(mtcars[,i]))\n}## [1] \"***** Summaries of mpg *****\"\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   10.40   15.43   19.20   20.09   22.80   33.90 \n## [1] \"***** Summaries of cyl *****\"\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   4.000   4.000   6.000   6.188   8.000   8.000 \n## [1] \"***** Summaries of disp *****\"\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    71.1   120.8   196.3   230.7   326.0   472.0 \n## [1] \"***** Summaries of hp *****\"\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    52.0    96.5   123.0   146.7   180.0   335.0 \n## [1] \"***** Summaries of drat *****\"\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   2.760   3.080   3.695   3.597   3.920   4.930 \n## [1] \"***** Summaries of wt *****\"\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   1.513   2.581   3.325   3.217   3.610   5.424 \n## [1] \"***** Summaries of qsec *****\"\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   14.50   16.89   17.71   17.85   18.90   22.90 \n## [1] \"***** Summaries of vs *****\"\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##  0.0000  0.0000  0.0000  0.4375  1.0000  1.0000 \n## [1] \"***** Summaries of am *****\"\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##  0.0000  0.0000  0.0000  0.4062  1.0000  1.0000 \n## [1] \"***** Summaries of gear *****\"\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   3.000   3.000   4.000   3.688   4.000   5.000 \n## [1] \"***** Summaries of carb *****\"\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   1.000   2.000   2.000   2.812   4.000   8.000"},{"path":"data.html","id":"data-requirement","chapter":"3 Data","heading":"3.2 Data Requirement","text":"","code":""},{"path":"data.html","id":"strategies-for-missing-data","chapter":"3 Data","heading":"3.3 Strategies for Missing Data","text":"","code":""},{"path":"data.html","id":"removing-observations","chapter":"3 Data","heading":"3.3.1 Removing observations","text":"large dataset percentage missing data small, can simply ignore observations. Note check whether pattern missingness “missing random,” meaning reason certain observations missing pure chance. , example, observations missing pollution level high low detection ideal, ignoring observations result biased conclusions.","code":""},{"path":"data.html","id":"imputation","chapter":"3 Data","heading":"3.3.2 Imputation","text":"Imputation commonly used method missing data. linearity plausible, can use linear imputation. response variables missing, Bayesian method can fill values using posterior draws. (words, process running analysis, algorithm can predict response variable fill .) Imputation big tool set , needed, consult comprehensive resource. [provide suggestions]","code":""},{"path":"evaluation.html","id":"evaluation","chapter":"4 Evaluation","heading":"4 Evaluation","text":"","code":""},{"path":"evaluation.html","id":"training-vs-testing","chapter":"4 Evaluation","heading":"4.1 Training vs Testing","text":"first address concepts training testing.","code":""},{"path":"evaluation.html","id":"metrics","chapter":"4 Evaluation","heading":"4.2 Metrics","text":"","code":""},{"path":"evaluation.html","id":"continuous-responses","chapter":"4 Evaluation","heading":"4.2.1 Continuous Responses","text":"","code":""},{"path":"evaluation.html","id":"discrete-responses","chapter":"4 Evaluation","heading":"4.2.2 Discrete Responses","text":"","code":""},{"path":"evaluation.html","id":"overfitting","chapter":"4 Evaluation","heading":"4.3 Overfitting","text":"","code":""},{"path":"evaluation.html","id":"cross-validation","chapter":"4 Evaluation","heading":"4.4 Cross Validation","text":"One standard way evaluation \\(k\\)-fold cross validation, commonly \\(k=5\\) \\(k=10\\). idea simple. Divide data \\(k\\) groups. time, choose \\(k-1\\) groups training, fit model last group, test data, calculate desired metrics, MSE.way, although less data used training, metrics accurate, now using data points training testing. Using metrics cross validation model selection can ensure model overfit, means model well training data generalize well new data.\nFigure 4.1: Image Source: https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n","code":""},{"path":"machine-learning-methods.html","id":"machine-learning-methods","chapter":"5 Machine Learning Methods","heading":"5 Machine Learning Methods","text":"provide list commonly used machine learning methods brief discussion.","code":""},{"path":"machine-learning-methods.html","id":"tree-based-methods","chapter":"5 Machine Learning Methods","heading":"5.1 Tree-based Methods","text":"Tree-based methods popular ease interpretation. Essentially, algorithm tries split data subsets finding significant (consequential) variables way reduces prediction error . example, graph , variable helps reduce prediction error first level gender. words, object male female can greatly help algorithm predict survival passengers Titanic.Decision trees easy use accurate complex data; however, can go beyond one tree. use multiple trees , instance, take average predictions trees, can improve performance. idea ensemble methods. One ensemble methods, random forests, commonly used, including natural resources management, quite accurate many kinds data. next ’ll dive one method.","code":""},{"path":"machine-learning-methods.html","id":"random-forests","chapter":"5 Machine Learning Methods","heading":"5.1.1 Random Forests","text":"random forests, algorithm randomly selects smaller number, say \\(m\\), predictors total number, say \\(p\\), time tree fit. process repeated many times, example, \\(500\\) times. average (continuous responses) majority votes (discrete responses) predictions individual trees used final predictions. randomly choosing subset predictors time, random forests can avoid relying variables every time prevent overfitting.Even though coefficients predictors available random forests, can easily get important variables least know variables important predicting response. can also use random forests tool variable selection use different method fit model subset predictors chosen random forests.[Need examples]","code":""},{"path":"machine-learning-methods.html","id":"other-ensemble-tree-methods","chapter":"5 Machine Learning Methods","heading":"5.1.2 Other Ensemble Tree Methods","text":"ensemble tree methods include boosting bagging. interested, take look [source].","code":""},{"path":"presentation.html","id":"presentation","chapter":"6 Presentation","heading":"6 Presentation","text":"also important present results way aids rather impede communication.","code":""},{"path":"presentation.html","id":"table","chapter":"6 Presentation","heading":"6.1 Table","text":"","code":""},{"path":"presentation.html","id":"figure","chapter":"6 Presentation","heading":"6.2 Figure","text":"","code":""},{"path":"ethical-considerations.html","id":"ethical-considerations","chapter":"7 Ethical Considerations","heading":"7 Ethical Considerations","text":"Ethics might sound heavy, section mainly ensuring analysis carried responsible way. concerns unique ML models, complexity ML methods makes ever important think issues.","code":""},{"path":"ethical-considerations.html","id":"reproducibility","chapter":"7 Ethical Considerations","heading":"7.1 Reproducibility","text":"allow others reproduce work, important provide enough details terms methods, data processing, code implementation, etc. also encouraged code data available online repository. parts data shared publicly, helps provide simulated data set.Another aspect reproducibility might surprising. Try use programming scripts rather drag--drop software. Starting reading data, write script read, clean, wrangle data. scripts can preserve steps data analysis. future, remember changed data arrived p-value, scripts can show details, drag--drop software leave trace.Note best practice read data programming environment make data cleaning merging script. can save cleaned version different file, don’t change file raw data.","code":""},{"path":"ethical-considerations.html","id":"decision-making","chapter":"7 Ethical Considerations","heading":"7.2 Decision making","text":"Since research related environmental sciences natural resources likely affect decision making, now address topics.","code":""},{"path":"ethical-considerations.html","id":"uncertain-qualification","chapter":"7 Ethical Considerations","heading":"7.2.1 Uncertain qualification","text":"ML models can provide point estimates, can quantify uncertainty. , however, important show confident model estimates. conclusion study affect important policy change, crucial present full picture findings, include uncertainty quantification. wildly different whether model 20% 95% confident answer, instance.","code":""},{"path":"ethical-considerations.html","id":"interpretability","chapter":"7 Ethical Considerations","heading":"7.2.2 Interpretability","text":"models, neural networks, highly efficient, like black boxes lend easily interpretablity. case neural networks, even able find weights hidden layers, really way interpret . ensure model arrives reasonable conclusion, might consider using model interpretable, linear regression tree-based methods. way, experts domain knowledge can examine whether conclusion makes sense. words, findings ML models can add researchers’ understanding field rather throwing answer easily interpreted.","code":""},{"path":"appendix.html","id":"appendix","chapter":"8 Appendix","heading":"8 Appendix","text":"","code":""},{"path":"appendix.html","id":"dos-and-donts","chapter":"8 Appendix","heading":"8.1 Do’s and Don’ts","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
