[["index.html", "Machine Learning Guidelines for Natural Resource Management Practitioners Chapter 1 Motivation", " Machine Learning Guidelines for Natural Resource Management Practitioners Shih-Ni Prim and Natalie Nelson 2024-03-07 Chapter 1 Motivation As machine learning (ML) has become a powerful tool, it is noted by some that ML has not been widely used in environmental studies. This booklet is meant to provide a concise guide for natural resource management practitioners. This book serves as a staring point rather than a comprehensive resource, so that practitioners can have a basic understanding of how ML works and how to utilize it to analyze data and answer research questions. When appropriate, we provide case studies and R code as well as other online resources to help the readers on the journey of gaining one powerful tool that seems to be omnipresent in the research world. "],["intro.html", "Chapter 2 Introduction 2.1 Supervised Learning 2.2 Unsupervised Learning", " Chapter 2 Introduction What is machine learning? Essentially, machine learning teaches computer models to look for patterns or make predictions. This might sound like magic or it might seem complicated, but you can think of machine learning models as finding underlying formulas that the data come from. To solve for such formula, many, many mathematical calculations are involved. As we human beings are prone to mistakes, as long as we can identify a framework, we can give the framework and data to a computer model. It is best at repeating meticulous calculations to find a best guess based on our believes of the system and the data we observed. Even though ML models are commonly considered black boxes, they are not necessarily so. Many ML methods lend easily to interpretation. Random forests, for example, is a powerful method that can offer a glimpse into what the important variables are, even if the straightforward explanation that can come from a linear model is not available. Below we introduce the two large branch of machine learning. Even though most of environmental studies probably fall into supervised learning, it is worth knowing that there is unsupervised learning, because it might help with some intermediate steps of studies. 2.1 Supervised Learning Simply put, supervised learning is when there is a true answer for the model. For example, if we want to use environmental traits (temperature, precipitation, chemical composition) to predict algal growth. In a dataset that record such activities, there are growths that are the right answers. The model can then use the predictors to make predictions, and the comparison between the predictions and the truth can show how well the model performs. 2.2 Unsupervised Learning Unsupervised learning, on the other hand, performs tasks that do not have a correct answer. For example, for a group of chemicals, some might have more similar traits than the others. Unsupervised learning can perform clustering to find such groupings, but it is still unsupervised because the grouping depends on the context. One could imagine that clustering can be performed first, and the groupings are used as the predictors instead of the chemicals themselves. This way the dimension can be reduced, assuming that the number of groups is smaller than the number of chemicals. "],["data.html", "Chapter 3 Data 3.1 Data Exploratoary Analysis 3.2 Data Requirement 3.3 Strategies for Missing Data", " Chapter 3 Data When you have data, don’t feel so rushed to jump into data analysis yet. Some steps can help you know your data better and, more than often, avoid problems down the road. 3.1 Data Exploratoary Analysis After you read in data, do some checks. Below we use the embedded mtcars as an example for illustration. library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors data(&quot;mtcars&quot;) str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... As seen above, the command str allows you to see the variables and their types. For this dataset, all variables are numerical. If some are categorical but they should be numerical, make sure you transform them into the right type of data. (Sometimes numbers can be saved as characters, and the analysis would not be correct if the datatype remains as character.) Next, try to make some plots–typically histograms for numerical variables and barplots for categorical variables. par(mfrow = c(3,3), mar = c(2,2,2,2)) for (i in 1:ncol(mtcars)){ hist(mtcars[,i], main = paste0(&quot;Histogram of &quot;, colnames(mtcars)[i])) } You can also look at the paired plots to see if two variables are too perfectly correlated, which could cause problems in regression models. pairs(mtcars) Next, take a look at the summary statistics. for (i in 1:ncol(mtcars)){ print(paste0(&quot;***** Summaries of &quot;, colnames(mtcars)[i],&quot; *****&quot;)) print(summary(mtcars[,i])) } ## [1] &quot;***** Summaries of mpg *****&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 10.40 15.43 19.20 20.09 22.80 33.90 ## [1] &quot;***** Summaries of cyl *****&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.000 4.000 6.000 6.188 8.000 8.000 ## [1] &quot;***** Summaries of disp *****&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 71.1 120.8 196.3 230.7 326.0 472.0 ## [1] &quot;***** Summaries of hp *****&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 52.0 96.5 123.0 146.7 180.0 335.0 ## [1] &quot;***** Summaries of drat *****&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.760 3.080 3.695 3.597 3.920 4.930 ## [1] &quot;***** Summaries of wt *****&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.513 2.581 3.325 3.217 3.610 5.424 ## [1] &quot;***** Summaries of qsec *****&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 14.50 16.89 17.71 17.85 18.90 22.90 ## [1] &quot;***** Summaries of vs *****&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.0000 0.4375 1.0000 1.0000 ## [1] &quot;***** Summaries of am *****&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.0000 0.4062 1.0000 1.0000 ## [1] &quot;***** Summaries of gear *****&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.000 3.000 4.000 3.688 4.000 5.000 ## [1] &quot;***** Summaries of carb *****&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 2.000 2.812 4.000 8.000 The summaries can show, for example, ranges and means of the variables. They can give you a better understanding of where the values are and whether there might be data entry errors. 3.2 Data Requirement 3.3 Strategies for Missing Data 3.3.1 Removing observations If you have a large dataset and the percentage of missing data is small, you can simply ignore those observations. Note that you should check whether the pattern of missingness is “missing at random,” meaning that the reason why certain observations are missing is pure chance. If, for example, some observations are missing because the pollution level is too high or too low and the detection is not ideal, then ignoring these observations could result in biased conclusions. 3.3.2 Imputation Imputation is a commonly used method for missing data. If linearity is plausible, you can use linear imputation. If some response variables are missing, some Bayesian method can fill in the values using posterior draws. (In other words, in the process of running the analysis, the algorithm can predict what the response variable should be and fill it in.) Imputation is a big tool set that, if needed, you should consult a more comprehensive resource. [provide suggestions] "],["evaluation.html", "Chapter 4 Evaluation 4.1 Training vs Testing 4.2 Metrics 4.3 Overfitting 4.4 Cross Validation", " Chapter 4 Evaluation 4.1 Training vs Testing We should first address the concepts of training and testing. Instead of using the entire data, it is common to split the dataset into a training set and a test set. Typically people use percentages such as 70% vs 30% or 80% vs 20%. The idea is to use only, say, 80% of the data for training, which means to run the select model on this part of data. Then you have a model with some coefficients (you can think of them as slopes) and test such results on the test set to see how well the model performs. Since the part of the data for testing is not used in training, this way there is no double dipping. Sometimes people split the dataset into three sets: training, test, and validation. So the trained models are used on the test set for model selection. Then the validation set is used only once at the end to gauge how the model should perform if there’s new data. 4.2 Metrics 4.2.1 Numerical Responses The most common metrics for numerical responses include mean squared errors (MSE), bias, and \\(R^2\\) (or adjusted \\(R^2\\)). The way to calculate MSE is: \\[MSE = \\frac{1}{n} \\sum_{i=1}^n \\bigg( \\hat{y}_i - y_i \\bigg)^2.\\] The metric shows, on average, the squared distance between predictions and true responses. Bias is calculated as \\[Bias = \\frac{1}{n} \\sum_{i=1}^n \\bigg( \\hat{y}_i - y \\bigg).\\] The two metrics seem similar, but they reveal different aspects of model performance. In general, bias shows if the predictions are centered around the truth, while the MSE shows how far the predictions are from the truth. If the bias is zero, it means that the mean prediction goes towards the truth as the sample size increases to infinity. This is naturally a desired property, but you might be surprised that sometimes we are willing to accept some bias to reduce the variance. One important formula, \\[MSE = Bias^2 + Variance,\\] where \\[Variance = \\frac{1}{n} \\sum_{i=1}^n \\bigg( \\hat{y}_i - E(\\hat{y}) \\bigg)^2\\] and \\[E(\\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n \\hat{y}_i.\\] We skip the derivation here, but the interested readers can find the derivation readily online. The formula tells us that, if the MSE is large but the bias is small, this means the model has a large variance. This is to say, if you fit the model multiple times, it has quite different results every time. These formulas might seem too technical, but the message is important. There is a trade off between the bias and variance; in other words, there is no free lunch. A balance between bias and variance is where many methods strive to achieve. 4.2.2 Categorical Responses When the responses are categorical, a common way to measure the performance is a confusion matrix, which shows the numbers of predictions that are correct or incorrect. [provide an example] Other metrics include sensitivity, specificity, precision, recall, and other metrics that try to combine these metrics to present a one-number summary. 4.3 Overfitting Overfitting means that the model can make good predictions with the available data, but it cannot generalize well on new data. It is kind of like if a student memorizes all the past exam questions without really studying the materials, then, on the exam, the student can do really well on questions coming from past exams but might do poorly on questions never seen before. Splitting data into training and testing is one strategy to avoid overfitting. Certain ML methods are less likely to overfit, such as neural networks. And there are some more stragegies, such as early stopping, that can be used. Our point here is to make you aware this potential pitfall of machine learning methods. 4.4 Cross Validation One very standard way of evaluation is \\(k\\)-fold cross validation, commonly with \\(k=5\\) or \\(k=10\\). The idea is simple. Divide the data into \\(k\\) groups. Each time, choose \\(k-1\\) groups for training, fit the model on the last group, which is the test data, and calculate the desired metrics, such as MSE. In this way, although less data is used for training, the metrics are more accurate, because now we are not using the same data points for training and testing. Using metrics from cross validation for model selection can ensure that your model does not overfit, which means the model does well with training data but does not generalize well on new data. Figure 4.1: Image Source: https://en.wikipedia.org/wiki/Cross-validation_(statistics) "],["machine-learning-methods.html", "Chapter 5 Machine Learning Methods 5.1 Tree-based Methods", " Chapter 5 Machine Learning Methods Here we provide a list of commonly used machine learning methods and some brief discussion. 5.1 Tree-based Methods Tree-based methods are popular for their ease for interpretation. Essentially, the algorithm tries to split the data into subsets by finding the most significant (or consequential) variables in a way that reduces the prediction error the most. For example, in the graph below, the variable that helps reduce prediction error at the first level is gender. In other words, the object being male or female can greatly help the algorithm predict the survival of passengers on Titanic. Image Source: https://en.wikipedia.org/wiki/Decision_tree_learning Decision trees are easy to use but not very accurate for complex data; however, we can go beyond one tree. If we use multiple trees and, for instance, take the average of the predictions of all the trees, we can improve the performance. This is the idea of ensemble methods. One of the ensemble methods, random forests, is commonly used, including in natural resources management, and quite accurate for many kinds of data. So next we’ll dive into this one method. 5.1.1 Random Forests For random forests, the algorithm randomly selects a smaller number, say \\(m\\), of predictors from the total number, say \\(p\\), each time a tree is being fit. This process is repeated many times, for example, \\(500\\) times. The average (for continuous responses) or the majority votes (for discrete responses) of the predictions from the individual trees are used as final predictions. By randomly choosing a subset of predictors each time, random forests can avoid relying on the same variables every time and prevent overfitting. Even though coefficients of each predictors are not available for random forests, you can easily get the important variables and at least know which variables are more important for predicting the response. You can also use random forests as a tool for variable selection and then use a different method to fit the model on a subset of predictors chosen by random forests. [Need examples] 5.1.2 Other Ensemble Tree Methods Other ensemble tree methods include boosting and bagging. If you are interested, take a look at [a source]. "],["presentation.html", "Chapter 6 Presentation 6.1 Table 6.2 Figure", " Chapter 6 Presentation It is also important to present the results in a way that aids rather than impede communication. 6.1 Table 6.2 Figure "],["ethical-considerations.html", "Chapter 7 Ethical Considerations 7.1 Reproducibility 7.2 Decision making", " Chapter 7 Ethical Considerations Ethics might sound heavy, but this section is mainly about ensuring that the analysis is carried out in a responsible way. These concerns are not unique to ML models, but the complexity of ML methods makes it ever more important to think about these issues. 7.1 Reproducibility To allow for others to reproduce your work, it is important to provide enough details in terms of methods, data processing, code implementation, etc. It is also encouraged to have all the code and data available online in a repository. If parts or all of the data should not be shared publicly, it helps to provide a simulated data set. Another aspect of reproducibility might be surprising. Try to use programming scripts rather than drag-and-drop software. Starting from reading in the data, write a script to read, clean, and wrangle with the data. The scripts can preserve all the steps of data analysis. In the future, when you cannot remember how you changed the data or how you arrived at a p-value, the scripts can show all the details, but drag-and-drop software will not leave any trace. Note that the best practice is to read in the data into the programming environment and make any data cleaning and merging in the script. You can save the cleaned version in a different file, but don’t change the file with the raw data. 7.2 Decision making Since research related to environmental sciences and natural resources could likely affect decision making, we will now address some topics. 7.2.1 Uncertain qualification While all ML models can provide point estimates, not all can quantify uncertainty. It is, however, important to show how confident the model is about the estimates. If the conclusion of the study could affect an important policy change, it is crucial to present a full picture of the findings, which include uncertainty quantification. It is wildly different whether the model is 20% or 95% confident about its answer, for instance. 7.2.2 Interpretability While some models, such as neural networks, are highly efficient, they are more like black boxes and do not lend easily to interpretablity. In the case of neural networks, even if you are able to find all the weights in the hidden layers, there is really no way to interpret them. To ensure that the model arrives at a reasonable conclusion, you might consider using a model that is more interpretable, such as linear regression or tree-based methods. This way, experts with domain knowledge can examine whether the conclusion makes sense. In other words, the findings from ML models can add to researchers’ understanding of the field rather than throwing out an answer that is not easily interpreted. "],["case-studies.html", "Chapter 8 Case Studies 8.1 Case Study 1: Machine learning approach for modeling daily pluvial flood dynamics in agricultural landscapes 8.2 Case Study 2: Short-term forecasting of fecal coliforms in shellfish growing waters 8.3 Case Study 3", " Chapter 8 Case Studies 8.1 Case Study 1: Machine learning approach for modeling daily pluvial flood dynamics in agricultural landscapes The full paper can be accessed here. 8.2 Case Study 2: Short-term forecasting of fecal coliforms in shellfish growing waters The full paper can be accessed here. 8.3 Case Study 3 "],["appendix.html", "Chapter 9 Appendix 9.1 Do’s and Don’ts", " Chapter 9 Appendix 9.1 Do’s and Don’ts "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
